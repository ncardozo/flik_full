% $Id: solution.tex 
% !TEX root = ../main.tex

\section{Flik: A Bug's Life Debugger}
\label{sec:solution}

\ac{ML} and \ac{RL} agents are normally built from off-the-shelf libraries and algorithms which posits 
a problem to understand abnormalities in agents' behavior. This problem is increased when libraries 
are black-box, as it is not possible to have fine-grained observations about the behavior of the 
libraries. In cases in which developers do have access to the learning behavior of \ac{RL} agents, 
debuggers emerge as an appropriate tool to understand said behavior. Moreover, given the intrinsic 
nature of continuous interaction between the agent and the environment, if the debugging 
process enables developers to modify programs' state, the evaluation of the agent's behavior is 
enhanced, as it is no longer required to stop-start-train the agent. Such property allows developers 
to experiment over the behavior of agents without having to go over the extensive process of 
stopping agents and going through the complete exploration with the new values.

\sidemargin{par:postmortem}{{\fref{rem:postmortem}}}
Seldomly, debuggers allow developers to go back in time to replay and analyze the program. 
Additionally, it is even rarer for debuggers to allow developers to 
modify variables. In order to contribute to the development of \ac{RL} programs, 
debugging tools should exhibit the aforementioned features, which we introduce in the following.


%%
\subsection{\flik in a nutshell}

To manage the complexity of \ac{RL} systems, and to mitigate the time consuming process of 
stopping-modifying-starting-training \ac{RL} agents to evaluate different learning parameters o 
algorithm values, the \flik back-in-time debugger is based on two main features:
\begin{description}
    \item[Stepping back:] Due to the extend of the execution loop in \ac{RL} programs, we 
    want to have a functionality that will allow developers to step back the execution to observe the 
    changes in the state between loop iterations, without having to re-start the complete execution.  
    This feature is adopted from omniscient debuggers.
    Stepping back will let developers interacting directly with the program, 
    avoiding the stop-modify-re-start process which causes to lose the program state, or the training 
    data already accumulated. Such feature will help in identifying the root cause of erroneous agent 
    behavior, whether that is an error in the program's design for particular interactions, or an ill-defined 
    hyperparameter. 
    \item [Modifying variables:] While stepping back into the program's execution, it should be possible 
    to modify variables' values to generate new traces of execution using the modified values. 
    Such feature would help developers to test out the behavior of an agent with different state-values, 
    or hyperparameters, without having to continuously stop and retrain the agent, which can be very 
    costly and time-consuming. This feature is adapted from timepoint debuggers.
\end{description}

These features aim at tackling the problem of interaction with the program, by stepping through the 
execution, both forward and backward, observing the effects of specific interactions between the agent 
and the environment under different conditions from states' and hyperparameters' values. The capacity 
to explore the execution allows developers to evaluate agents' behavior and the quality of the \ac{RL} 
programs, without having to retrain every time, and therefore helping to improve the development of 
these programs.

Anchored in these features, \flik: \textsc{A bug's life}, is a back-in-time debugger with timepoints 
to modify variables' values, and resume execution on a different execution path using the new values. 
Specifically, for the case of \ac{RL} agents this means that using \flik it is 
possible to: evaluate the internal state of the agent, the decisions it makes, and the rewards 
it receives, through the observation of agents' variables and hyperparameters over time. Using the 
debugger, developers can better understand the execution context of \ac{RL} agents in terms of 
variables, values, environment, states and the rewards. 

\begin{figure}[hptb]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/flik_interface.png}
    \caption{The \flik debugger interface with its main frames to observe the program execution and its internal state}
    \label{fig:debugger}
\end{figure}

\fref{fig:debugger} shows \flik's interface working to debug a simple example of the \spy{bubble_sort} 
function. From the top, the first frame (in the blue rectangle - Execution frame) we have the running 
execution, in this case the print for the array to be sorted in Line 018 in the second frame. The second 
frame (in the purple rectangle - Source code frame) shows the source code being debugged. The third 
frame (in the red rectangle - Variables frame) shows the variables used in the program so far. Finally, 
in the bottom of the interface we have the interactive console in which developers can send 
commands to \flik to effectively debug the program.

\sidemargin{par:gridworld}{{\fref{rem:gridworld}}}
We now use a running example to present the features and commands of \flik. Four 
our running example we use the Gridworld \ac{RL} benchmark, presented in detail in 
\fref{sec:evaluation} and shown in \fref{fig:gridworld} as a $10 \times 10$ grid. 

\begin{figure}[hptb]
  \centering
  \includegraphics[width=0.4\columnwidth]{figures/gridworld.png}
  \caption{10x10 gridworld environment example}
  \label{fig:gridworld}
\end{figure}

For the purpose of the running example, we will use the Q-learning implementation, shown in 
\fref{lst:gridworld-learner}, for the agent to execute in the Gridworld environment, explaining the \flik 
commands used to detect a bug in the code. Running the program, we pay special attention to the 
\spy{run} function, which we can access using the \spy{n} command until we reach it. Alternatively, it 
is possible to move directly to a specific execution point, by setting breakpoints in the program. For 
example, when executing the program, we can use the \spy{c} command to execute until the 
\spy{breakpoint} in \fref{ln:breakpoint} is reached. Once in the desired function, we use the \spy{n} 
command to step through it. At particular points in the execution (\eg \fref{ln:stop1}), we can observe 
variables' values by means of the \spy{p} o \spy{pp} commands. In our example, we use 
\spy{p self.gamma} to see the value of the \spy{gamma} hyperparameter. 

Continuing with the execution for a couple of episodes to let the agent train, we can use the 
\spy{variables} command, in \fref{ln:stop2}, to observe the values of the different variables. Upon 
inspection, we have the following observations. First, the historic value of the \spy{action} is $0$ 
throughout the program execution. Second, all values in the \spy{qtable} are unchanged (all continue 
as 0). Third, the value of the \spy{epsilon} hyperparameter is $0.1$. 
Putting both values together, we note that from the condition in \fref{ln:stop1}, the agent has a 
high probability of tacking the else branch exploiting the known values. However, as the agent has 
not trained updating the values, the exploitation is equivalent to random action choosing, and 
therefore leading to erroneous agent behavior.

\begin{python}[numbers=left,
	caption={\flik running example of the Gridworld environment},
	label={lst:gridworld-learner}]
class Agent:
  def __init__(self, env, alpha=0.1, gamma=0.6, epsilon=0.1, episodes=10):
    #hyperparameters
    self.alpha = alpha
    self.gamma = gamma
    self.epsilon = epsilon
    self.environment = env
    self.qtable = self.__initdic__() #initialize q-table
    self.episodes = episodes
    
  def run(self, ui_input):
      done = False
      counter = 0
      breakpoint()  ` \label{ln:breakpoint} `
      for i in range(self.episodes):
        done = False
        while not done:
          current_state = copy.deepcopy(self.environment.state) `\label{ln:back1}`
          if random.uniform(0,1) < self.epsilon:   ` \label{ln:stop1} `
            action = self.random_action(current_state)
          else:
            action = np.argmax(self.qtable[current_state])  
          next_state, reward, done, info = self.step(action) ` \label{ln:stop2} `
          old_value = self.qtable[current_state][action]
          next_max = np.max(self.qtable[next_state])
          new_value = (1-self.alpha)*old_value + self.alpha*(reward+self.gamma*next_max)
          self.qtable[current_state][action] = new_value
          counter += 1
        if ui_input == 1:
          actions, values = self.actions_values()
          self.environment.plot_action(actions, values)
        self.environment.reset()
    return self.qtable
\end{python}

During the execution we can use \flik to confirm that the root cause of the problem is the value of the 
\spy{epsilon} hyperparameter. To do this, from \fref{ln:stop2} we can use the command 
\spy{back_to 18} to go backward in the program execution to \fref{ln:back1}. Then, we can effectively 
change the value of \spy{epsilon} using the \spy{setvar self.epsilon=0.9} command. Once the change 
is made, if we continue execution (in a different \flik branch), after reaching \fref{ln:stop2}, we can print 
the value of the \spy{action} variable (\spy{p action}), to observe now different actions are taken, 
leading to proper agent training, taken from the different values across the q-table.\footnote{A video of 
this example execution of \flik is available at \url{https://youtu.be/PkkhT76IYrU?si=vqBjUCSfwRUhEZnJ}}

\fref{tab:flik-commands}, shows a summary of the commands implemented in \flik, where the 
first six commands in the first block are specific to \flik, and the following 12 commands in the second 
block are provided with \ac{PDB}. Note from the implementation that, variable evaluation (\spy{p}) and 
variable value updates (\spy{setvar}) can alternatively be executed directly without using the 
corresponding commands.

\begin{table}
  \centering
  \caption{Description of \flik commands}
  \input{tables/flik-commands}
  \label{tab:flik-commands}
\end{table}


%%
\subsection{\flik internals}
\label{sec:flik-internals}

\sidemargin{par:requirements2}{{\fref{rem:requirements}, \ref{rem:implementation}\ref{rem:implementation2}}}
We now turn our attention to the implementation of \flik and its inner workings. \flik is a console-based 
debugger, built on top of \ac{PDB}, as shown in the general \flik model in 
\fref{fig:flik-architecture}. 

\begin{figure}[hptb]
  \centering
  \includegraphics[width=1\columnwidth]{figures/flik-architecture}
  \caption{\flik's architecture and internal details}
  \label{fig:flik-architecture}
\end{figure}

\flik reuses many of the functionality available in PDB to instrument and execute Python programs, 
and adds features 
such as colored syntax highlighting, tracking of variables' state, and capturing stdout output 
from executed lines of code. The following are the three major features of \flik:
\begin{itemize}
    \item Stores the state in each execution step (\ie action taken by the agent). \flik saves the local 
    and global variables, in a history variable. Additionally, other metadata like the information of the 
    line being executed is saved. This allows us to later step back into specific states in the stored 
    history, and their corresponding location in the code. 
    \item Restores a previous program state as, from the previously stored information, from the 
    stack and heap variables to precisely restore the program's state and its execution context, in 
    order to re-create an alternative execution path. 
    \item Finally, connecting the previous two features, is the action of stepping back. This feature is 
    created as a custom \ac{PDB} command, using the same syntax and form of the native \ac{PDB} 
    commands (\fref{tab:flik-commands}). The stepping back command takes the state saved at a specific point in the history 
    and restores the state according to the information at that point --that is, program's state, stack 
    information, and code line. 
\end{itemize}

In Python, the internal state of a program during execution is primarily encapsulated in 
stack frames. Each stack frame contains information about the execution state of a function 
call, including the current line number, local and global variables, and other metadata, as follows:
\begin{itemize}
    \item \spy{f_lineno}: The current line number being executed.
    \item \spy{f_locals}: A dictionary of local variables within the frame.
    \item \spy{f_globals}: A dictionary of global variables accessible within the frame.
    \item \spy{f_code}: A code object representing the function's bytecode and source code metadata.
\end{itemize}

The state is saved in a list that stores snapshots of the frame's state (line and variables) at each 
execution step, which allows stepping back into any state we would want to restore. We use the  
\spy{exec} function that receives the stored \spy{f_globals} and \spy{f_locals} variables as 
parameters. \spy{exec} then executes a list of stored state snapshots (line number 
and local variables) at each step in a specified frame's context (given by the function parameters). 
This allows \flik to simulate running a specific line in the context of a previous frame, maintaining 
both local and global variable references, essentially restoring the saved execution state.

This all done by extending the \ac{PDB} class and adding custom commands to support stepping 
back and a custom interface which allows the user to interact with the debugger. The interface 
displays the code, variables, and execution point, and allows the user (to use the \ac{PDB} 
functions) to pause, step forward, step back, continue or restart the program, as well as to 
modify and inspect variables. 

\endinput

video \url{https://drive.google.com/file/d/1NyipuWsRr6ZrIbtlvU5qyooHS2aVsWXc/view?usp=sharing}.