% $Id: solution.tex 
% !TEX root = ../main.tex

\section{Flik: A Bug's Life Debugger}
\label{sec:solution}

\ac{ML} and \ac{RL} agents are normally built from off-the-shelf libraries and algorithms which posits 
a problem to understand abnormalities in agents' behavior. This problem is increased when libraries 
are black-box, as it is not possible to have fine-grained observations about the behavior of the 
libraries. In cases in which developers do have access to the learning behavior of \ac{RL} agents, 
debuggers emerge as an appropriate tool to understand said behavior. Moreover, given the intrinsic 
nature of continuous interaction between the agent and the environment, if the debugging 
process enables developers to modify programs' state, the evaluation of the agent's behavior is 
enhanced, as it is no longer required to stop-start-train the agent. Such property allows developers 
to experiment over the behavior of agents without having to go over the extensive process of 
stopping agents and going through the complete exploration with the new values.

\sidemargin{par:postmortem}{{\fref{rem:postmortem}}}
Seldomly, debuggers allow developers to go back in time to replay and analyze the program. 
Additionally, it is even rarer for debuggers to allow developers to 
modify variables. In order to contribute to the development of \ac{RL} programs, 
debugging tools should exhibit the aforementioned features, which we introduce in the following.


%%
\subsection{\flik in a nutshell}
\label{sec:flik-nutshell}

To manage the complexity of \ac{RL} systems, and to mitigate the time consuming process of 
stopping-modifying-starting-training \ac{RL} agents to evaluate different learning parameters o 
algorithm values, the \flik back-in-time debugger is based on two main features:
\begin{description}
    \item[Stepping back:] Due to the lengthly execution loop in \ac{RL} programs, we 
    want to have a functionality that will allow developers to step back the execution to observe the 
    changes in the state between loop iterations, without having to re-start the complete execution.  
    This feature is adopted from omniscient debuggers.
    Stepping back will let developers interacting directly with the program, 
    avoiding the stop-modify-re-start process which causes to lose the program state, or the training 
    data already accumulated. Such feature will help in identifying the root cause of erroneous agent 
    behavior, whether that is an error in the program's design for particular interactions, or an ill-defined 
    hyperparameter. 
    \item [Modifying variables:] While stepping back into the program's execution, it should be possible 
    to modify variables' values to generate new traces of execution using the modified values. 
    Such feature would help developers to test out the behavior of an agent with different state-values, 
    or hyperparameters, without having to continuously stop and retrain the agent, which can be very 
    costly and time-consuming. This feature is adapted from timepoint debuggers.
\end{description}

These features aim at tackling the problem of interaction with the program, by stepping through the 
execution, both forward and backward, observing the effects of specific interactions between the agent 
and the environment under different conditions from states' and hyperparameters' values. The capacity 
to explore the execution allows developers to evaluate agents' behavior and the quality of the \ac{RL} 
programs, without having to retrain every time, and therefore helping to improve the development of 
these programs.

Anchored in these features, \flik: \textsc{A bug's life}, is a back-in-time debugger with timepoints 
to modify variables' values, and resume execution on a different execution path using the new values. 
Specifically, for the case of \ac{RL} agents this means that using \flik it is 
possible to: evaluate the internal state of the agent, the decisions it makes, and the rewards 
it receives, through the observation of agents' variables and hyperparameters over time. Using the 
debugger, developers can better understand the execution context of \ac{RL} agents in terms of 
variables, values, environment, states and the rewards. 

\begin{figure}[hptb]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/flik_interface.png}
    \caption{The \flik debugger interface with its main frames to observe the program execution and its internal state}
    \label{fig:debugger}
\end{figure}

\fref{fig:debugger} shows \flik's interface working to debug a simple example of the \spy{bubble_sort} 
function. From the top, the first frame (in the blue rectangle - Execution frame) we have the running 
execution, in this case the print for the array to be sorted in Line 018 in the second frame (the current 
execution frame). The second frame (in the purple rectangle - Source code frame) shows the source 
code being debugged. The third frame (in the red rectangle - Variables frame) shows the variables 
used in the program so far. Finally, in the bottom of the interface we have the interactive console in 
which developers can send commands to \flik to effectively debug the program.

\sidemargin{par:gridworld}{{\fref{rem:gridworld}}}
We now use a running example to present the features and functionality/commands of \flik. Four 
our running example we use the Gridworld \ac{RL} benchmark, presented in detail in 
\fref{sec:evaluation} and shown in \fref{fig:gridworld} as a $10 \times 10$ grid. 

\begin{figure}[hptb]
  \centering
  \includegraphics[width=0.4\columnwidth]{figures/gridworld.png}
  \caption{10x10 gridworld environment example}
  \label{fig:gridworld}
\end{figure}

For the purpose of the running example, we will use the Q-learning implementation, shown in 
\fref{lst:gridworld-learner}, for the agent to execute in the Gridworld environment, explaining the \flik 
commands used to detect a bug in the code. Running the program, we pay special attention to the 
\spy{run} function, which we can access using the \spy{n} command until we reach it. Alternatively, it 
is possible to move directly to a specific execution point, by setting breakpoints in the program. For 
example, when executing the program, we can use the \spy{c} command to execute until the 
\spy{breakpoint} in \fref{ln:breakpoint} is reached. Once in the desired function, we use the \spy{n} 
command to step through it. At particular points in the execution (\eg \fref{ln:stop1}), we can observe 
variables' values by means of the \spy{p} or \spy{pp} commands. In our example, we use 
\spy{p self.gamma} to see the value of the \spy{gamma} hyperparameter. 

Continuing with the execution for a couple of episodes to let the agent train, we can use the 
\spy{variables} command, at \fref{ln:stop2} of the program, to observe the values of the different 
variables. Upon inspection through multiple iterations, we have the following observations. First, the 
value of the \spy{action} variable is $0$ throughout the program execution. Second, all values in the 
\spy{qtable} are unchanged (all continue as 0). Third, the value of the \spy{epsilon} hyperparameter is 
$0.1$. Putting all these values together, we note that from the condition in \fref{ln:stop1}, the agent 
has a high probability of taking the else branch exploiting the known values by the agent. However, 
as the agent has not trained updating the values, the action taken by the agent is random,
leading to erroneous agent behavior (\ie not learning a proper policy).

\begin{python}[numbers=left,
	caption={\flik running example of the Gridworld environment},
	label={lst:gridworld-learner}]
class Agent:
  def __init__(self, env, alpha=0.1, gamma=0.6, epsilon=0.1, episodes=10):
    #hyperparameters
    self.alpha = alpha
    self.gamma = gamma
    self.epsilon = epsilon
    self.environment = env
    self.qtable = self.__initdic__() #initialize q-table
    self.episodes = episodes
    
  def run(self, ui_input):
      done = False
      counter = 0
      breakpoint()  ` \label{ln:breakpoint} `
      for i in range(self.episodes):
        done = False
        while not done:
          current_state = copy.deepcopy(self.environment.state) `\label{ln:back1}`
          if random.uniform(0,1) < self.epsilon:   ` \label{ln:stop1} `
            action = self.random_action(current_state)
          else:
            action = np.argmax(self.qtable[current_state])  
          next_state, reward, done, info = self.step(action) ` \label{ln:stop2} `
          old_value = self.qtable[current_state][action]
          next_max = np.max(self.qtable[next_state])
          new_value = (1-self.alpha)*old_value + self.alpha*(reward+self.gamma*next_max)
          self.qtable[current_state][action] = new_value
          counter += 1
        if ui_input == 1:
          actions, values = self.actions_values()
          self.environment.plot_action(actions, values)
        self.environment.reset()
    return self.qtable
\end{python}

During the execution we can use \flik to confirm that the root cause of the problem is the value of the 
\spy{epsilon} hyperparameter. To do this, from \fref{ln:stop2} we can use the command 
\spy{back_to 18} to go backward in the program execution to \fref{ln:back1}. Then, we can  
change the value of \spy{epsilon} using the \spy{setvar self.epsilon=0.9} command. Once the change 
is made, if we continue execution (in a different \flik branch), after reaching \fref{ln:stop2}, we can print 
the value of the \spy{action} variable (\spy{p action}), to observe now different actions are taken, 
leading to proper agent training, taken from the different values across the q-table.\footnote{A video of 
this example execution of \flik is available at \url{https://youtu.be/PkkhT76IYrU?si=vqBjUCSfwRUhEZnJ}}

\fref{tab:flik-commands}, shows a summary of the commands implemented in \flik, where the 
first six commands (on the top of the table) are specific to \flik, and the following 12 commands (at the 
bottom of the table) are provided by the \ac{PDB} debugger. 
%Note from the implementation that, variable evaluation (\spy{p}) and variable value updates (\spy{setvar}) can alternatively be executed directly without using the corresponding commands.

\begin{table}
  \centering
  \caption{Description of \flik commands}
  \input{tables/flik-commands}
  \label{tab:flik-commands}
\end{table}


%%
\subsection{\flik internals}
\label{sec:flik-internals}

\sidemargin{par:requirements2}{{Remarks \ref{rem:requirements}, \ref{rem:implementation}, \ref{rem:implementation2}}}
Having introduced the features of \flik and their use debug \ac{RL} programs using the different 
commands, we now turn our attention to \flik's implementation and its inner workings.\footnote{\flik is available for download at: \url{https://github.com/FLAGlab/flik} and distributed as a Docker image at: \url{https://hub.docker.com/r/lrodriguez22/py-flik-debugger}.}  
\flik is a console-based debugger built on top of \ac{PDB}, adding features such as colored syntax 
highlighting and tracking of variables' state. \ac{PDB}, in turn, is built on top of the Python 
runtime enabling access to run-time variables and state as provided by the Python interpreter. A 
diagram of \flik's architecture is shown in \fref{fig:flik-architecture}.

\begin{figure}[hptb]
  \centering
  \includegraphics[width=1\columnwidth]{figures/flik-architecture}
  \caption{\flik's architecture and internal details}
  \label{fig:flik-architecture}
\end{figure}

To debug a Python program with \flik, we reuse the functionality already available in \ac{PDB} to 
instrument the code of a running interpreted program. Using the instrumentation available for Python 
programs lets \flik work with the full Python specification without having to build an interpretar and 
program instrumentation from scratch. 
\sidemargin{par:python}{{Remarks \ref{rem:python-io}, \ref{rem:python-full}, \ref{rem:python-lib}}}
As such \flik work with external libraries and capture stdout 
output I/O operations from the execution. The access to external libraries when running \flik is divided 
in two groups, depending on the nature of the libraries. For libraries available locally in the system, 
\flik has full access to the code, and therefore all of \flik's capabilities are available, as with any other 
regular Python file from the program. For libraries remote to the system, \flik does not have access to 
the source code and setting breakpoints or timepoints for code within the library is not possible. 
Therefore, in the latter case stepping back into the libraries' code is not possible, nor is it possible to 
modify library variables' values.

To support the two features put forward for \flik, we implement the functionality to: 
\begin{enumerate*}[label=(\arabic*)]
\item capture the program state, 
\item step-back in the execution, 
\item restore a given state, and 
\item update the values of the program state.
\end{enumerate*}
These functions work with the internal representation of state in \flik. State is represented by means 
of a buffer as shown in \fref{fig:flik-architecture}
\sidemargin{par:state}{{\fref{rem:state}}}

The functionality of \flik extends the \ac{PDB} class by adding the required custom commands to 
step back, and customizing the interface to interact with the debugger. The interface 
displays the code, variables, and execution point, and allows the user (to use the \ac{PDB} 
functions) to pause, step forward, step back, continue or restart the program, as well as to 
modify and inspect variables. 

\paragraph{Capture program state.}
Storing the program state is a key functionality to realize the two features. As mentioned previously, 
\flik stores the system state for each execution step (\ie action taken by the agent) in a history buffer. 
The information saved in the history buffer includes all global and local variables' values after the 
execution of a step, together with metadata related to the executed expression, as for example its 
line number. System state is encapsulated in stack frames in Python, which can be captured using 
the \spy{inspect.stack()} function. Each stack frame contains different information related to the 
execution state of a function call, for which we save:
\begin{itemize}
    \item \spy{f_locals}: A dictionary of local variables within the frame.
    \item \spy{f_globals}: A dictionary of global variables accessible within the frame.
    \item \spy{f_code}: A code object representing the function's bytecode and source code metadata.
    \item \spy{f_lineno}: The current line number being executed metadata.
\end{itemize}

Upon a function execution, we put the frame containing all state values and metadata into the history 
buffer. To mitigate the impact on the memory overhead of storing the information of all executed 
functions we restrict the history buffer to have a fixed size (XXXMB in our current setting). As a 
consequence, the buffer keeps the 100 most recent history frames, removing the oldest frame in the 
history when the buffer is full.

\paragraph{Step-back.}    
The history buffer is used to enable stepping back. In an execution function, the step back commands 
(\spy{step_back} or \spy{back_to}) take the history buffer and search the corresponding saved state. 
Stepping to the previous saved state (when \spy{step_back} is used), or searching through the buffer 
for a specific execution line within the saved metadata (when \spy{back_to} is used).

\paragraph{Restore program state.}    
Once the program execution is at a specific frame in the history buffer, to resume the execution from 
the given point, \flik must restore the saved state. Restoring a program state sets back all local and 
global variables' values, and their associated execution context, to the values stored at the current 
frame in the history buffer.

A direct consequence of restoring a previous program state is that the execution resumes in a new 
execution path. Note that currently \flik does not keep track of the different execution path, as there is 
no clear way to navigate them without the use of a GUI. To restore the sate, we use the \spy{exec} 
Python function that receives the stored \spy{f_globals} and \spy{f_locals} variables as 
parameters. \spy{exec} then executes a list of stored state snapshots (line number 
and local variables) at each step in a specified frame's context (given by the function parameters). 
This allows \flik to simulate running a specific line in the context of a previous frame. Additionally, 
restoring a given state, clears the more recent frames from the history buffer to start the new 
execution path.

\paragraph{Update state values.}
Finally, in stepping back and restoring a program state, \flik makes it possible to modify the value of 
any of the stored variables (using the \spy{setvar} command). At a given execution point, updating 
the value of a variable restores the program state, creating a new execution path with the new 
values. 

\flik creates a casual connection between the Python variables stored in memory and the variables 
stored in the history buffer, so that the changes to variables in \flik percolate down to the variables 
stored in Python's memory.

Updating state values is a key feature of \flik to manage \ac{RL} programs as it enables developers 
to explore different execution paths to observe the behavior of the agent under multiple 
circumstances. Using this feature it is possible to modify learned values or learning parameters in 
specific environment states to assess the behavior of the agent, without having to retrain the agent 
every time. 


\endinput

video \url{https://drive.google.com/file/d/1NyipuWsRr6ZrIbtlvU5qyooHS2aVsWXc/view?usp=sharing}.