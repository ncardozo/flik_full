% $Id: problem.tex 
% !TEX root = ../main.tex

\section{\acl{RL} Basics}
\label{sec:background}

\sidemargin{par:background}{{\fref{rem:intro}}}
\ac{RL}~\cite{sutton18} encompasses different artificial intelligence techniques and algorithms in 
which an agent learns the behavior to reach a goal by specializing its interaction with the 
environment in which it is deployed.
\ac{RL} agents learn the state-action function for a given policy through interaction with the 
environment to discover an optimal solution to reach a goal, through the optimal policy $\pi^*$ (a set 
of actions to execute at each system state) gathered from the interaction of the agent with the 
environment. One of the most common algorithms in \ac{RL} is Q-learning~\cite{beakcheol19}, which 
uses an off-policy control that separates the deferral policy from the learning policy and updates the 
action selection using the Bellman optimal equation, and the $\epsilon$-greed policy.  In the definition 
of \ac{RL} algorithms, the environment is composed of a set of states $S$, a set of actions $A$ the agent can execute. By performing an action $a\in A$ at state $s \in S$, the agent transitions from $s$
to a new state $s' \in S$. Additionally, associated to the execution of the action $a$ at state $s$, the 
agent receives a reward $r$. The goal of the agent is to maximize its total reward. \fref{lst:qlearning} 
presents the base (pseudo) algorithm for Q-learning~\cite{sutton18}.

\begin{algorithm}
\caption{Q-Learning Algorithm}\label{lst:qlearning}
\begin{algorithmic}
\Require An environment with states $S$, actions $A$, and reward function $R(s, a)$
\Require A learning rate $\alpha \in [0, 1]$
\Require A discount factor $\gamma \in [0, 1]$
\Require Exploration strategy (\eg $\epsilon$-greedy)
\State Initialize Q-table $Q(s, a) \gets 0$ for all $s \in S$, $a \in A$
\For{each episode}
    \State Initialize the starting state $s_0$
    \For{each time step in the episode}
        \If{exploration step}
            \State Choose an action $a$ using the exploration strategy (e.g., $\epsilon$-greedy)
        \Else
            \State Choose an action $a$ based on the current Q-values: $a = \arg\max_a Q(s, a)$
        \EndIf
        \State Take action $a$, observe reward $R(s,a)$ and next state $s'$
        \State Update Q-value: 
        \[
  \qquad  Q(s, a) \gets Q(s, a) + \alpha \left( R(s,a) + \gamma \max_{a'} Q(s', a')\right)
        \]
        \State Set $s \gets s'$
    \EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

\ac{RL} has gained wide popularity in the last couple of years, finding applications in a wide range of 
domains~\cite{beakcheol19}, particularly in control of industrial process~\cite{kiumarsi18} (improving 
the performance of the on-line learning control system or optimizing temperature control and power 
consumption), computer networks~\cite{alrawi13} (improving adaptability of Wireless Sensor Networks 
to changing situations and eliminating the need for system redesign), and robotics~\cite{zhang15} (by 
providing frameworks and toolkits for designing sophisticated behavioral aspects). 




\endinput

