% $Id: state_of_the_art.tex 
% !TEX root = ../main.tex

\section{State of the Art}
\label{sec:state_of_the_art}

This section presents the state of the art from three perspectives:, the main debuggers in other 
programming languages, existing Python debuggers, debuggers for \ac{AI} programs in general, 
including debuggers for \ac{RL} programs specifically.


%%
\subsection{Debuggers for other programming languages}
\label{sec:other}

We first turn our attention to the state of the art in debugging tools available for different 
programming languages. There are many debugging tools available for different programming 
languages, each with its own set of features and capabilities, most of them integrated with IDEs as
built-in debugging tools that allow developers to inspect programs' behavior during their execution. 
However, there are also standalone debugging tools that can be used with different 
programming languages, and that offer more advanced features and capabilities than the 
built-in debugging tools of IDEs. 

Some of the most popular debugging tools include \ac{GDB}~\cite{stallman11}, 
which stands for GNU Project Debugger and is a powerful 
debugging tool for C and C++ (although it also supports other programming languages,
like Ada, Go, or Rust). GDB helps developers to inspect the internals of C programs while 
they are executing. \ac{GDB} works on binary executable files produced during compilation. During 
execution, \ac{GDB} allows developers to observe exactly what happens when a program 
crashes. \ac{GDB} consists of three major subsystems~\cite{stallman11}. 
\begin{enumerate*}[label=(\arabic*)] 
\item The user interface subsystems, consists of several actual interfaces, plus their supporting code. 
\item The symbol handling subsystem, consists of object file readers, debugging info interpreters, 
symbol table management, source language expression parsing, and type and value printing. 
\item The target system handling consists of execution control, stack frame analysis, and 
physical target manipulation.
\end{enumerate*}

DeloreanJs~\cite{leger23} is a back-in-time debugger for JavaScript programs. The value of 
back-in-time debuggers is the possibility to rewinding execution to a specific point, allowing the 
possibility for developers to, for instance, test different variable values within the same execution 
context, to better understand errors or explore hypothetical scenarios in the program's execution 
evolution~\cite{hofer06,lienhard09}. DeloreanJs has three main features: the ability to navigate 
through an execution history, modify values of a variable, and resume execution from a timepoint (at 
specific execution points in the past)~\cite{leger23}. DeloreanJS extends continuations with static 
analysis functionalities to capture the current program state and store the (history of) variable's 
values. There are several other debuggers for Javascript, most of them based on the use of 
breakpoints, in which variables cannot be modified to resume current execution with the new values, 
similar to the behavior offered in \ac{GDB}.

We note the functionality to go back in time, observe the program's state at a previous moment and 
execute a new timeline with the new values, offered by DeloreanJS, and in general back-in-time 
debuggers, is desirable in the context of \ac{RL} programs. Therefore, we use  back-in-time 
debuggers, and DeloreanJS, as a reference to create our debugger for \ac{RL} programs.


%%
\subsection{Python debuggers}
\label{sec:py}

Given that  most \ac{RL} programs are written in Python, we pay special attention to existing 
debuggers for Python. There are several Python debuggers available. We focus on those debuggers
that have support for the features required to debug \ac{RL} programs, such as the ability to navigate 
through an execution history (like a time travel debugger), and the ability to modify the values
of variables and resume the execution from a specific point in time.

% PDB
The \ac{PDB}~\cite{python-pdb} debugger is the built-in Python Debugger in CPython~\cite{shaw21} 
(the standard Python bytecode interpreter), implemented by several IDEs that provide a missing 
visual interface. \ac{PDB} is a powerful interactive source code debugger for Python programs, 
implemented as a module that can be used in Python scripts. \ac{PDB} provides a command-line 
interface for debugging Python programs, allows developers to define breakpoints, step through 
program instructions, inspect variables, and evaluate expressions. Additionally, \ac{PDB} provides 
a post-mortem debugging feature that allows developers to debug a program after it has crashed. 
However, \ac{PDB} does not have the ability to navigate through the execution history of a program, 
modify values of variables, or resume execution from a specific point in time, which are the features 
we are looking for in our solution. 

% PuDB
PuDB~\cite{pudb} is a full-screen, console-based visual debugger for Python. PuDB allows 
developers to set breakpoints, step through code, inspect variables, and evaluate expressions. 
Similar to \ac{PDB}, PuDB also provides a post-mortem debugging feature that allows developers to 
debug a program after it has crashed. However, PuDB does not have the ability to navigate through 
a program's execution history, modify values of variables, or resume execution from a specific point 
in time. Nevertheless, the history navigation feature can be simulated placing breakpoints in the 
code and restarting the execution until the breakpoint is reached. The UI makes it possible to go 
through the code and see the variables and the stack trace in a user-friendly fashoin.

% PyTrace
PyTrace~\cite{pytrace} is a time travel recorder/analyzer for Python. PyTrace records code execution, 
variables and stack frames. It has a UI that allows developers to navigate through the code, 
see the variables and the stack trace, and it is very user-friendly. However, PyTrace is not 
a full-featured debugger, as it does not allow you to change the values of variables, or do 
any other operation available to debuggers. PyTrace works as a tool to understand the behavior of 
a program, moving backward and forward through a recorded execution (again in a single execution 
path),  with different  graphs to see the state of your variables at a moment in time.

% RevPDB
\ac{RevPDB}~\cite{revdeb} is a reverse debugger for Python programs, RevPDB is an extension of 
\ac{PDB} built on top of the PyPy interpreter, allowing developers to go forward and backward in time. 
However, RevPDB is not a full-featured debugger, nor it has a user-friendly UI, as it is a proof of 
concept of the time traveling features, and it is not currently maintained.


% UDB
\ac{UDB}~\cite{udb} is a proprietary debugger, developed by the Company Undo, for Python 
programs that allows developers to navigate through time and inspect the state of the program at a 
previous moment in time. \ac{UDB} works on the principle of first recording 
a program while it is running normally and then replaying the execution while allowing the 
developer to navigate and inspect the program's state at different points in time. \ac{UDB} works at 
the process level, rewinding and replaying the state of the entire process. Technically, 
\ac{UDB} does allow new code paths to be executed at replay-time, but all execution replays are 
isolated.

\fref{tab:python-debuggers} shows a comparative summary of existing Python debuggers, and the 
features they offer. 

\begin{table}[hptb]
  \centering
  \caption{Comparative summary of existing Python debuggers.}
  \input{tables/python-debuggers}
  \label{tab:python-debuggers}
\end{table}


%%
\subsection{Debuggers for \ac{AI} programs}
\label{sec:ai}

We now discuss the debuggers or debugger-like tools available for \ac{AI} and \ac{ML} programs. 
Identifying the root causes of bugs in machine learning systems is a complex task. Some bugs may 
originate within the models themselves, and their black-box nature makes pinpointing faults especially 
difficult. Additionally, the code used to train or run these models might be flawed, adding another layer 
of complexity. In systems that rely on \ac{ML} pipelines, bugs may not reside in the individual 
components but rather in how these components interact. Hardware miss-configurations are also a 
potential source of faults. Unfortunately, traditional debuggers embedded in IDEs are limited when 
detecting \ac{ML}-specific bugs. Therefore, there is an increasing interest on techniques and tools for 
debugging and testing \ac{ML} models.  

\ac{ML} programs are often debugged using visualization tools that allow developers to observe 
the program's behavior and analyze their internal state. The advance on \ac{ML} debugging tools 
represent an important step in towards  visualization and debugging tools for \ac{RL} programs.
Most of the existing tools are postmortem, not allowing developers to interact with the 
program during execution; therefore, making the tools unsuitable to analyze \ac{RL} programs and 
the continuous interaction between the environment and the agent.

CockPit~\cite{schneider21} is an open source debugging tool designed for deep neural network 
training, enabling practitioners to observe internal training dynamics in real time —much like 
traditional debuggers, it provides insights into program execution. Rather than relying solely on loss 
and accuracy curves, Cockpit offers a suite of instruments that monitor higher-order metrics such as 
gradient statistics, curvature, and sharpness, which are now efficiently computable during training. 
These insights help identify algorithmic failure modes—like unsuitable learning rates or optimization 
plateaus—and support more informed adjustments. Available as an open-source PyTorch package, 
Cockpit enhances both the interpretability and explainability of deep learning processes by revealing 
previously opaque aspects of training behavior.

TensorFlow offers several tools for debugging tensorFlow programs such as TensorFlow Debugger 
and TensorBoard Debugger~\cite{tensorboard}. The former allows for interactive execution of 
TensorFlow graphs, setting breakpoints, and inspecting tensor values in real-time.  The debugger 
can also be used in monitoring mode to  logs various aspects (at runtime) of a tensor flow program 
by instrumenting it with API calls.  The debugger can be combined with the TensorBoard to have 
visualizations/inspectors of the program state such as alerts, python execution timeline, graph 
executions, stack trace, and a source code viewers. However, TensorBoard does not allow 
developers to interact with the model during execution, which is essential for debugging \ac{RL}.

\ac{Vizarel}~\cite{deshpande20} is a debugger for \ac{RL}. \ac{Vizarel} implements an interactive 
visualization tool for debugging and interpreting \ac{RL} 
programs. The system offers different views that encapsulate the spatial and temporal dimensions 
of agent policies. The tool consists of a set of \emph{viewport modules}, each of which is an 
abstract entity that can be backed by different specs, conditioned on the underlying 
data~\cite{deshpande20}. This tool is useful to understand the behavior of an agent. However, it 
does not allow the user to interact with the variables during execution, which leads to a longer training 
process, having to wait until the end of the training run to know whether the agent is learning correctly 
or not.

\citet{rajan23} present a tool called \ac{MDP} Playground. This platform is intended to help 
developers and researchers to understand \ac{RL} agents better on toy complex environments, and 
to create unit tests characterizing agents' behavior  on toy \ac{MDP} examples, based on the 
OpenGym library. The problem addressed by \ac{MDP} Playground is the fragility of \ac{RL} 
programs, especially with the growth of deep \ac{RL} research growing. The complexity of the 
dynamics of \ac{RL} programs increases, leading to difficulties in identifying states or rewards 
distribution; if the agent's policy is not as expected, debugging \ac{RL} programs becomes 
hard~\cite{rajan23}. The idea with \ac{MDP} Playground is to open the programs, making their 
internal state available, to understand the behavior of the agent. Based only on toy examples, 
\ac{MDP} Playground is not scalable, nor it is suitable for real-world applications, but a 
proof-of-concept good to understand the behavior of \ac{RL} agents. While the \ac{MDP} Playground 
is not a debugger, it can help to debug the program by understanding the behavior of the agent in a 
controlled environment.

Finally, another recent work to debug \ac{RL} programs, sets as its purpose to gain trust in \ac{AI} 
programs using systematic testing~\cite{steinmetz21}. Erroneous program behavior (\ie bugs) lead to 
a bad agent policy, for example, not reaching the goal, or not learning the correct policy. The purpose 
of the work is to provide a systematic testing approach to find bugs. As part of the process, the tool 
offers a methodology to find potential bugs, or confirm a bug exists, using the Framework Fuzzing.

While the approaches presented are interesting to help understand \ac{RL} programs, none of them 
are actual debuggers, leaving a gap in the current literature, which is addressed as the main focus of 
our work. Our work proposes a debugger that allows the user to navigate through the execution 
history, modify the values of variables, and resume the execution from a specific point in time, 
performs into creating better \ac{RL} programs.
Open source and commercial tools like CockPit, DeepKit, TensorFlow Debugger, Amazon SageMaker 
Debugger,  TensorWatch, and Neptune.ai offer different features to debug/monitor ML systems. 
However, to the best of our knowledge, there is no open source option to debug  
\ac{RL} programs. 


\endinput

 


