% $Id: results.tex 
% !TEX root = ../main.tex

\section{Analysis and Results}
\label{sec:results}

This section presents the analysis of the results of the empirical study presented in 
\fref{sec:evaluation}. The results are presented according to three sections in the survey: General 
knowledge (\fref{sec:general-knowledge}), Task Effectiveness (\fref{sec:tasks-results}) and Usability 
(\fref{sec:usability}). Finally, a discussion section has the analysis of the results form the survey. All 
data obtained from the evaluation is available as part of the data and replication package and the 
online appendix (\url{https://flaglab.github.io/Flik_Experiments/}).
All responses are available 
at \url{https://shorturl.at/DhN56}. 

%%
\subsection{General Knowledge Results}
\label{sec:general-knowledge}

Most of the participants were very experienced with programming in Python as it can be seen in 
(\fref{fig:python-experience}). The experience level of 5 is selected as participants use Python very 
often in their work and university courses. Similarly, most participants are well familiarized with 
\ac{RL} (\fref{fig:rl-experience}), as they were taking a course on the topic. Note that locating the 
bugs in the programs required familiarity with \ac{RL} and Python knowledge. Furthermore, the 
participants reported being familiar  with using the terminal, and therefore, interacting with \flik, using 
the commands and the interface would neither pose a problem nor require a learning curve 
(\fref{fig:terminal-experience}). Nevertheless, in average, the participants were not familiarized with 
debuggers, and only used them  rarely, normally using Visual Studio Code interface's simple 
features, like graphical breakpoint functionality. This leads to misunderstandings and some difficulties 
in completing the tasks, as the  tool is based on command line interaction to stop and step through 
the code (\fref{fig:debugging-experience}). This characteristic from the participants caused a steeper 
learning curve to use the tool, than expected from participants with previous debugger experience.

\begin{figure}[hptb]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/experience-python}
        \caption{Experience using Python}
        \label{fig:python-experience}
    \end{subfigure}
    ~ 
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/experience-rl}
        \caption{Experience in \ac{RL}}
        \label{fig:rl-experience}
    \end{subfigure}
    ~ 
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/experience-terminal}
        \caption{Experience using the terminal}
        \label{fig:terminal-experience}
    \end{subfigure}
    ~ 
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/experience-debuggers}
        \caption{Experience in debugging}
        \label{fig:debugging-experience}
    \end{subfigure}
    \caption{Experience reported by users in the four main dimensions of the tools used in the evaluation}
    \label{fig:general-know}
\end{figure}


%%
\subsection{Tasks Results}
\label{sec:tasks-results}
In \fref{fig:general-know} the y-axis represents the number of participants registering 
answers to the question at the given Likert score. The x-axis corresponds to the score, with
values in the range from 5 to 1 (from completely agree, to completely disagree). In the last 
question (the third bar in green for all graphs), 5 represents taking a lot of time, and 1 represents 
taking very little time.

Taking the results reported from the three tasks, most of the participants completed the first task successfully (\fref{fig:task1}). Participants reported the task was easy to solve. Even though the task 
was reported to be easy, it took participants a long time to complete, because this was the task in 
which participants had to get familiarized with the tool. 

The second task was harder than the first one, as participants were not familiar with the problem 
(\ie Rooms), and its complexity is higher (when compared to Gridworld). Most participants took 
longer to finish task 2 (\fref{fig:task2}) and some struggled finding the root cause of the bug in the 
program.  Such result is expected, as the bug introduced in task 2, does not exhibit a completely 
wrong agent behavior, but a more subtle interaction with the environment and the obtained Q-values. 
Moreover, the fix required more than updating a value, but actually modifying the learning function. 
Such required change confused to some participants. 

Finally, the third task (\fref{fig:task3}) was the hardest one, most participants had trouble finding the 
bug. The general comment about the task is that it was not easy to solve. Nevertheless, the 
participants managed to finish the task in less time than in the other tasks. 

From the results it is expected that identifying and fixing the bug in the first task took longer than for 
other tasks, even though this is the easiest task, participants were still getting familiarized with the 
tool, and the learning curve for debugging the programs is steep. In the following tasks, while the 
nature of the introduced bugs is different, there is a learning curve  with using the tool. For the second 
task, given the nature of the bug and the fact that the bug was introduced in the Q-learning algorithm's 
logic, it was harder to identify. Hence, this task was harder to solve, even though it was completed by 
most participants. Finally, for the third task, the introduced bug had yet a different nature, in the reward 
function. For these task the learning curve in using the tool was more developed, and it took 
participants less time than expected for this reason.


\begin{figure}[hptb]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/task1}
        \caption{Gridworld task}
        \label{fig:task1}
    \end{subfigure}
    ~ 
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/task2}
        \caption{Rooms task}
        \label{fig:task2}
    \end{subfigure}
    ~ 
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/task3}
        \caption{Cars task}
        \label{fig:task3}
    \end{subfigure}
    \caption{Results of the tasks to solve}
    \label{fig:general-know}
\end{figure}


%%
\subsection{Debugger Usability Results}
\label{sec:usability}

The results for  the debugger's usability, shown in \fref{tab:general1-debuggers} and 
\fref{tab:general2-debuggers}, exhibit a sense that the tool is useful. The comments, in general,  
express that \flik would be useful, especially if given more time to study it, and gain practice with the 
tool before using it in real tasks. The tool was easy to use, once familiarized with the commands and 
their use. Additionally, there are several comments about providing a GUI or improving the current UI,
to help understanding how the tool works, and how to navigate the code with it, as at it differs from 
more mainstream debuggers like the one available with Visual Studio Code and its interface.

\begin{table}[H]
  \centering
  \caption{General Results Part 1}
  \input{tables/usability1}
  \label{tab:general1-debuggers}
\end{table}

\begin{table}[H]
  \centering
  \caption{General Results Part 2}
  \input{tables/usability2}
  \label{tab:general2-debuggers}
\end{table}

% \begin{table}[]
% \centering
% \input{tables/usability3}
% \end{table}

In Tables \ref{tab:general1-debuggers} and \ref{tab:general2-debuggers} the scale represents the 
Likert score option, expressing whether the participants completely agree ($5$) or completely disagree 
($1$) with the question at hand. The values inside the tables correspond to the number of participants 
that reported each given scale.

%%
\subsection{Performance and scalability}
\label{sec:scalability}

\sidemargin{par:scalability}{{\fref{rem:scalability}}}
This section presents the performance and scalability of running \flik on top of the \ac{RL} 
application, versus the execution of just the \ac{RL} application in the case of the three \ac{RL} 
applications (Gridworld, Rooms, and Driving Assistant). The evaluation takes into account the 
performance of the application in terms of required memory, and delay of executed \flik operations. 
The scalability is considered with respect to the size of the state space of the applications evaluated. 




%%
\subsection{Discussion}
\label{sec:discussion}

In general, the results of the empirical evaluation are very positive. Most of the participants have a 
good impression of \flik, finding it to be useful, especially for the kind of challenges presented in 
\ac{RL} programs. While participants found it hard to familiarize themselves with the tool, especially 
for the participants with little experience with debuggers, at the beginning of the evaluation session, 
most of the participants found the tool easy to use (after some initial use). Additionally, participants 
mentioned the tool be of use in general for the development of \ac{RL} programs in general, for example 
to develop course work, identifying the errors in the study as recurrent during development. The 
participants to the empirical study  also highlighted \textbf{\flik is useful in detecting common and hard to identify errors and misbehavior in \ac{RL} programs}.

With respect to the usability dimension the participants reported that, with some difficulties at the 
beginning, again for the participants without previous experience with debuggers, 
\textbf{\flik is usable}. The usability of the tool is confirmed by the completion of the three tasks by a 
majority of the participants, and the rapid learning curve gathered when switching between tasks, 
reaching a faster and more fluent code navigation to inspect the program, identify the cause of the 
error, and fix the bug, as reported in the time taken to complete each task. Nonetheless, the 
participants suggested that to increase the usability of \flik \textit{it is desirable to implement more GUI 
features closer to common features used in other debuggers, as the Visual Studio Code debugger}.


%%
\subsection{Threats to validity}
\label{sec:threads}

\sidemargin{par:threads}{{\fref{rem:threads}}}
To conclude the discussion of the results and the experiment we present the threads to validity to 
our experimentation and the conclusions extracted from it.

\begin{description}[leftmargin=0.1cm, labelindent=0.1cm]
\item[Threats to external validity] refer to the generalization of the results from our evaluation. 
\flik is designed as a debugger for \ac{RL} programs enabling developers to inspect the internal state 
of their agents and the environment state while they execute. In doing so, developers can change the 
values of variables, and observe agent behavior from any point in the execution. Our evaluation and 
user study explores the usability of \flik in the case of discrete agents based on the Q-learning 
algorithm. From the results, we confirm \flik's usability in such cases, and can generalize the results 
to other discrete algorithms such as SARSA or Multi-armed-bandit~\cite{lattimore20}. However, 
generalizing the usability to continuous \ac{RL} environments requires further evaluation in such 
environments, in which the intervention of developers withe the variables and the possibility to 
observe all environments variables and parameters may be limited due to the sheer volumen of 
variables, and the use of external \acl{DNN} libraries \flik may not have access to.

\item[Threats to construct validity] refer to the cases in which the evaluation is not exhaustive 
or selective enough. Our validation is built on the user study of \flik. In the user study we strived to 
gather users that have at least some basic experience with \ac{RL}. From the open call to the 
evaluation, the user study had 27 participants. While the participants are all students, they do have
diverse profiles, maturity level, and experience in working with \ac{RL}. Therefore, we can state that 
there is no bias in the selection of the participants for the user study.

In addition, the design of the user study was such that the first exercise is familiar to all developers 
with \ac{RL} experience, used to help participants familiarize themselves with \flik (after the 
presentation of \flik in a small example). The following tasks in the study are of less familiarity for 
\ac{RL} developers, the last being a completely new application, to evaluate the effectiveness of \flik 
in helping developers to understand \ac{RL} agents' behavior, mitigating any previous knowledge 
developers may have about possible errors in the application. There is no bias about the 
application selection favoring the usability of \flik over other applications. However, we highlight again 
the need to evaluate \flik in continuous environments.

\item[Threats to internal validity] refers to the conditions that may have an impact on the 
phenomena under test. The user study was set so that all participants could execute under the 
same conditions: characteristics of the machines in which the evaluation was executer, information 
and documentation of \flik, access to \flik experts, and time constraints to work on the different 
applications, reducing any bias that may have appear in the execution of the user study 
compromising the results.
\end{description}


\endinput

